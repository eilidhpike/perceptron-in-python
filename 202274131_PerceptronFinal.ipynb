{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Part 1: Perceptron\n",
    "\n",
    "CS802 Assignment <br>\n",
    "202274131 <br>\n",
    "\n",
    "### Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 - Complete the implementation of the Perceptron and use it to identify the clothes/shoes type (shirt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The perceptron is implemented as a class\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, no_inputs, max_iterations=20,learning_rate=0.1): #Initalising the perceptron\n",
    "        self.no_inputs = no_inputs\n",
    "        self.weights = np.ones(no_inputs + 1) / (no_inputs + 1) #Weights of the features stored in a vector, plus 1 which ensures that they are stored eqaully \n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "    #=======================================#\n",
    "    # Prints the details of the perceptron. #\n",
    "    #=======================================#\n",
    "    def print_details(self): #Printing details of the perceptron\n",
    "        print(\"No. inputs:\\t\" + str(self.no_inputs))\n",
    "        print(\"Max iterations:\\t\" + str(self.max_iterations))\n",
    "        print(\"Learning rate:\\t\" + str(self.learning_rate))\n",
    "    \n",
    "    def step(self, a): #Activation function - step function. \n",
    "        return np.where(a >= 0, 1, 0) #returns 1 if 'a' is greater than or eqaul to 1, and returns zero otherwise\n",
    "        return step\n",
    "    \n",
    "    #=========================================#\n",
    "    # Performs feed-forward prediction on one #\n",
    "    # set of inputs.                          #\n",
    "    #=========================================#\n",
    "    \n",
    "    def predict(self, x, activation): #feed forward prediction \n",
    "        a = np.dot(x, self.weights) #takes a set of inputs 'x', calculates the weighted sum of inputs by taking the dot product and current weights of the input \n",
    "        output = activation(a) #passes the weighed sum 'output' through the activation function \n",
    "        return output #returns the output of the perceptron\n",
    "\n",
    "    #======================================#\n",
    "    # Trains the perceptron using labelled #\n",
    "    # training data.                       #\n",
    "    #======================================# \n",
    "    \n",
    "    def train(self, training_data, labels):\n",
    "        assert len(training_data) == len(labels)\n",
    "    \n",
    "        # Add bias to training data\n",
    "        training_data = np.concatenate((training_data, np.ones((len(training_data), 1))), axis=1)\n",
    "    \n",
    "        # Combine training data and labels into a list of tuples\n",
    "        data = list(zip(training_data, labels))\n",
    "    \n",
    "        # Shuffle the data\n",
    "        np.random.shuffle(data)\n",
    "    \n",
    "        # Update weights for each training sample\n",
    "        for i in range(self.max_iterations):\n",
    "            for x, y in data:\n",
    "                output = self.predict(x, self.step)\n",
    "                error = y - output\n",
    "                self.weights += self.learning_rate * error * x\n",
    "    \n",
    "        return\n",
    "\n",
    "    #=========================================#\n",
    "    # Tests the prediction on each element of #\n",
    "    # the testing data. Prints the precision, #\n",
    "    # recall, and accuracy of the perceptron. #\n",
    "    #=========================================#\n",
    "    def test(self, testing_data, labels):\n",
    "        assert len(testing_data) == len(labels)\n",
    "        testing_data = np.concatenate((testing_data, np.ones((len(testing_data), 1))), axis=1) #Adding a column of ones to the training data to act as a bias\n",
    "        true_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        for x, y in zip(testing_data, labels):\n",
    "            output = self.predict(x, self.step)\n",
    "            if output == 1 and y == 1:\n",
    "                true_positives += 1\n",
    "            elif output == 1 and y == 0:\n",
    "                false_positives += 1\n",
    "            elif output == 0 and y == 1:\n",
    "                false_negatives += 1\n",
    "            elif output == 0 and y== 0:\n",
    "                true_negatives += 1\n",
    " #The above code creates an output using the predit and step functions. It checks the output by comparing the output to the labels.   \n",
    "        accuracy = (true_positives + true_negatives) / (false_positives + false_negatives + true_positives + true_negatives)\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        print(\"Accuracy:\\t\"+str(accuracy))\n",
    "        print(\"Precision:\\t\"+str(precision))\n",
    "        print(\"Recall:\\t\"+str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Fashion MNIST Dataset\n",
    "\n",
    "The Fashion MNIST Dataset was created by the Modified National Institute of Standards and Technology. It is a dataset of images of clothing, with 60000 training images and 10000 testing images. Each image is 28x28 pixels and has a corresponding label. The dataset has become a commonly used benchmark in machine learning models in image classification, and is significantly harder to classify than the orignal MNIST dataset which are handwritten digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the test and train data\n",
    "\n",
    "train_data = np.loadtxt(\"fashion_mnist_train.csv\", delimiter=\",\")\n",
    "test_data = np.loadtxt(\"fashion_mnist_test.csv\", delimiter=\",\")\n",
    "\n",
    "#Reshaping the data with rows and columns eqaul to the total number of pixel values in each dataset. \n",
    "train_data = train_data.reshape((train_data[:, 1:].shape[0], -1)) / 255.0\n",
    "test_data = test_data.reshape((test_data[:,1:].shape[0], -1)) / 255.0\n",
    "\n",
    "l = np.arange(10)\n",
    "#Creates numpy array of class labels\n",
    "train_labels = (np.asfarray(train_data[:, :1]))\n",
    "test_labels = (np.asfarray(test_data[:, :1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. inputs:\t785\n",
      "Max iterations:\t20\n",
      "Learning rate:\t0.1\n"
     ]
    }
   ],
   "source": [
    "#A separate cell creates a Perceptron\n",
    "p = Perceptron(28*28+1)\n",
    "p.print_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Complete.\n",
      "Testing...\n",
      "Accuracy:\t0.9462\n",
      "Precision:\t0.6708579881656804\n",
      "Recall:\t0.907\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "training_data = train_data\n",
    "labels = [d[0]==0 for d in train_labels]\n",
    "p.train(training_data, labels)\n",
    "print(\"Complete.\")\n",
    "\n",
    "# Testing the node\n",
    "print(\"Testing...\")\n",
    "testing_data = test_data\n",
    "labels = [d[0]==0 for d in test_labels]\n",
    "p.test(testing_data, labels)\n",
    "print(\"Complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** <br>\n",
    " Using the step activation function and online learning, the perceptron achieved an accuracy of 0.9462, correctly classiying the 95% of the data. The perceptron achieved an precision of 0.6708, which means that when the model predicted the image was a false positive, it was correct 67% of the time. THe recall of the model was 0.907, which means the model correctly identifed true positives 90.7% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Update the perceptron implementation to use full batch learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full batch learning<br>\n",
    "\n",
    "Full batch method learning updates the model's weights based on the average of the gradients of the entire dataset. All training examples are examined at the same time, then the average of the loss is used to update the weights (Alpaydin, 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The perceptron is implemented as a class\n",
    "class pFullBatch(object):\n",
    "    \n",
    "    def __init__(self, no_inputs, max_iterations=20,learning_rate=0.1):\n",
    "        self.no_inputs = no_inputs\n",
    "        self.weights = np.ones(no_inputs + 1) / (no_inputs + 1)\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "    #=======================================#\n",
    "    # Prints the details of the perceptron. #\n",
    "    #=======================================#\n",
    "    def print_details(self):\n",
    "        print(\"No. inputs:\\t\" + str(self.no_inputs))\n",
    "        print(\"Max iterations:\\t\" + str(self.max_iterations))\n",
    "        print(\"Learning rate:\\t\" + str(self.learning_rate))\n",
    "    \n",
    "    def step(self, a):\n",
    "        return np.where(a >= 0, 1, 0) #\n",
    "        return step\n",
    "    \n",
    "    #=========================================#\n",
    "    # Performs feed-forward prediction on one #\n",
    "    # set of inputs.                          #\n",
    "    #=========================================#\n",
    "    \n",
    "    def predict(self, x, activation):\n",
    "        a = np.dot(x, self.weights)\n",
    "        output = activation(a)\n",
    "        return output\n",
    "\n",
    "    #======================================#\n",
    "    # Trains the perceptron using labelled #\n",
    "    # training data.                       #\n",
    "    #======================================# \n",
    "    \n",
    "    # Full batch learning\n",
    "    def train(self, training_data, labels):\n",
    "        assert len(training_data) == len(labels)\n",
    "        training_data = np.concatenate((training_data, np.ones((len(training_data), 1))), axis=1)\n",
    "        for i in range(self.max_iterations):\n",
    "            outputs = self.predict(training_data, self.step)\n",
    "            errors = labels - outputs\n",
    "            delta = self.learning_rate * np.dot(training_data.T, errors)\n",
    "            self.weights += delta\n",
    "        return\n",
    "    #=========================================#\n",
    "    # Tests the prediction on each element of #\n",
    "    # the testing data. Prints the precision, #\n",
    "    # recall, and accuracy of the perceptron. #\n",
    "    #=========================================#\n",
    "    def test(self, testing_data, labels):\n",
    "        assert len(testing_data) == len(labels)\n",
    "        testing_data = np.concatenate((testing_data, np.ones((len(testing_data), 1))), axis=1)\n",
    "        true_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        for x, y in zip(testing_data, labels):\n",
    "            output = self.predict(x, self.step)\n",
    "            if output == 1 and y == 1:\n",
    "                true_positives += 1\n",
    "            elif output == 1 and y == 0:\n",
    "                false_positives += 1\n",
    "            elif output == 0 and y == 1:\n",
    "                false_negatives += 1\n",
    "            elif output == 0 and y== 0:\n",
    "                true_negatives += 1\n",
    "    \n",
    "        accuracy = (true_positives + true_negatives) / (false_positives + false_negatives + true_positives + true_negatives)\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        print(\"Accuracy:\\t\"+str(accuracy))\n",
    "        print(\"Precision:\\t\"+str(precision))\n",
    "        print(\"Recall:\\t\"+str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. inputs:\t785\n",
      "Max iterations:\t20\n",
      "Learning rate:\t0.1\n"
     ]
    }
   ],
   "source": [
    "#A separate cell creates a Perceptron\n",
    "pFB = pFullBatch(28*28+1)\n",
    "pFB.print_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Complete.\n",
      "Testing...\n",
      "Accuracy:\t0.9475\n",
      "Precision:\t0.7367896311066799\n",
      "Recall:\t0.739\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "training_data = train_data\n",
    "labels = [d[0]==0 for d in train_labels]\n",
    "pFB.train(training_data, labels)\n",
    "print(\"Complete.\")\n",
    "\n",
    "# Testing the node\n",
    "print(\"Testing...\")\n",
    "testing_data = test_data\n",
    "labels = [d[0]==0 for d in test_labels]\n",
    "pFB.test(testing_data, labels)\n",
    "print(\"Complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** <br>\n",
    " Using the step activation function and full batch learning, the perceptron achieved an accuracy of 0.9475, meaning it correctly classified 94.75% of the data. The precision of the model was 0.7367, which means that when the model predicted a false positive, it was correct 73.67% of the time. The recall of the model was 0.739, which means the model correctly identified true positives 73.9% of the time.\n",
    " \n",
    "he full batch learning model achieved an accuracy of 0.9475, which is slightly higher than the accuracy achieved by the online learning model (0.9462). The full batch learning model also achieved a higher precision (0.736) compared to the online learning model (0.6708). However, the recall of the full batch learning model (0.739) is slightly lower than the recall of the online learning model (0.907). Therefore, if we compare the perceptron results using full batch learning and online learning, we can see that full batch learning resulted in a higher precision score than online learning, indicating that the model was more selective in its positive predictions. However, the recall score was lower for full batch learning, indicating that the model missed more positive instances than in the case of online learning. This is to be expected due to the precision-recall curve. The implementation of full batch learning appears to have shfited the curve to the right (higher precision, but lower recall). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Use multiple nodes to classify every clothes/shoes type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node for label 0 ...\n",
      "Complete.\n",
      "Training node for label 1 ...\n",
      "Complete.\n",
      "Training node for label 2 ...\n",
      "Complete.\n",
      "Training node for label 3 ...\n",
      "Complete.\n",
      "Training node for label 4 ...\n",
      "Complete.\n",
      "Training node for label 5 ...\n",
      "Complete.\n",
      "Training node for label 6 ...\n",
      "Complete.\n",
      "Training node for label 7 ...\n",
      "Complete.\n",
      "Training node for label 8 ...\n",
      "Complete.\n",
      "Training node for label 9 ...\n",
      "Complete.\n",
      "Testing node for label 0 ...\n",
      "Accuracy:\t0.9483\n",
      "Precision:\t0.6987654320987654\n",
      "Recall:\t0.849\n",
      "Complete.\n",
      "Testing node for label 1 ...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting node for label\u001b[39m\u001b[38;5;124m\"\u001b[39m, i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39mi \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m test_labels]\n\u001b[1;32m---> 17\u001b[0m \u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#nodes[3].test(testing_data, labels)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mPerceptron.test\u001b[1;34m(self, testing_data, labels)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#The above code creates an output using the predit and step functions. It checks the output by comparing the output to the labels.   \u001b[39;00m\n\u001b[0;32m     80\u001b[0m        accuracy \u001b[38;5;241m=\u001b[39m (true_positives \u001b[38;5;241m+\u001b[39m true_negatives) \u001b[38;5;241m/\u001b[39m (false_positives \u001b[38;5;241m+\u001b[39m false_negatives \u001b[38;5;241m+\u001b[39m true_positives \u001b[38;5;241m+\u001b[39m true_negatives)\n\u001b[1;32m---> 81\u001b[0m        precision \u001b[38;5;241m=\u001b[39m \u001b[43mtrue_positives\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_positives\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfalse_positives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m        recall \u001b[38;5;241m=\u001b[39m true_positives \u001b[38;5;241m/\u001b[39m (true_positives \u001b[38;5;241m+\u001b[39m false_negatives)\n\u001b[0;32m     84\u001b[0m        \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(accuracy))\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Create 10 nodes, one for each clothing/shoe type\n",
    "nodes = [Perceptron(no_inputs=785) for i in range(10)]\n",
    "\n",
    "# Train all nodes to recognize all types of clothes/shoes\n",
    "for i in range(10):\n",
    "    training_data = [np.append([1],d[1:]) for d in train_data]\n",
    "    print(\"Training node for label\", i, \"...\")\n",
    "    labels = [d[0]==i for d in train_labels]\n",
    "    nodes[i].train(training_data, labels)\n",
    "    print(\"Complete.\")\n",
    "\n",
    "# Test all nodes\n",
    "for i in range(10):\n",
    "    testing_data = [np.append([1],d[1:]) for d in test_data]\n",
    "    print(\"Testing node for label\", i, \"...\")\n",
    "    labels = [d[0]==i for d in test_labels]\n",
    "    nodes[i].test(testing_data, labels)\n",
    "    #nodes[3].test(testing_data, labels)\n",
    "    print(\"Complete.\")\n",
    "\n",
    "# Iterate through the testing set and print the prediction for each node\n",
    "for d in testing_data:\n",
    "    input_data = np.concatenate(([1], d[1:]))\n",
    "    predictions = [nodes[i].predict(input_data, nodes[i].step) for i in range(10)]\n",
    "    prediction = predictions.index(max(predictions))\n",
    "    print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "Unfortunetly, this code did not work as expected. I believe that the nodes are being trained on class 0 only, and this is why the first test is successful, but the others are not. After attempting to debug this multiple times, regretfully, I have not been able to solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. Use the sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Activation \n",
    "\n",
    "The sigmoid function is a logistic function that maps any real-valued number to a value between 0 and 1. The output is an S shaped curve on a graph. The sigmoid activation is used in binary classification problems where the output is either 0 or 1. However, the sigmoid function can cause weights to become stuck at a local maximum, therefore, it is predicted that the sigmoid activation will reduce either the precision or the recall score in comparison to the full batch and step functions (Pratiwi, H. and Rahadjeng, I.R, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pSigmoid(object):\n",
    "    \n",
    "    def __init__(self, no_inputs, max_iterations=20, learning_rate=0.1):\n",
    "        self.no_inputs = no_inputs\n",
    "        self.weights = np.ones(no_inputs + 1) / (no_inputs + 1)\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def print_details(self):\n",
    "        print(\"No. inputs:\\t\" + str(self.no_inputs))\n",
    "        print(\"Max iterations:\\t\" + str(self.max_iterations))\n",
    "        print(\"Learning rate:\\t\" + str(self.learning_rate))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def predict(self, x, activation):\n",
    "        a = np.dot(x, self.weights)\n",
    "        output = activation(a)\n",
    "        return output\n",
    "    \n",
    "    def train(self, training_data, labels):\n",
    "        assert len(training_data) == len(labels)\n",
    "        training_data = np.concatenate((training_data, np.ones((len(training_data), 1))), axis=1) #add the bias\n",
    "        data = list(zip(training_data, labels))  #combine the randomised data into a list of tuples. \n",
    "    \n",
    "        # Shuffle the data\n",
    "        np.random.shuffle(data)\n",
    "    \n",
    "        # Update weights for each training sample\n",
    "        for i in range(self.max_iterations):\n",
    "            for x, y in data:\n",
    "                output = self.predict(x, self.sigmoid)\n",
    "                error = y - output\n",
    "                self.weights += self.learning_rate * error * output * (1 - output) * x\n",
    "    \n",
    "        return\n",
    "\n",
    "    def test(self, testing_data, labels):\n",
    "        assert len(testing_data) == len(labels)\n",
    "        testing_data = np.concatenate((testing_data, np.ones((len(testing_data), 1))), axis=1)\n",
    "        true_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        for x, y in zip(testing_data, labels):\n",
    "            output = self.predict(x, self.sigmoid)\n",
    "            if output >= 0.5 and y == 1:\n",
    "                true_positives += 1\n",
    "            elif output >= 0.5 and y == 0:\n",
    "                false_positives += 1\n",
    "            elif output < 0.5 and y == 1:\n",
    "                false_negatives += 1\n",
    "            elif output < 0.5 and y== 0:\n",
    "                true_negatives += 1\n",
    "\n",
    "        accuracy = (true_positives + true_negatives) / (false_positives + false_negatives + true_positives + true_negatives)\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        print(\"Accuracy:\\t\"+str(accuracy))\n",
    "        print(\"Precision:\\t\"+str(precision))\n",
    "        print(\"Recall:\\t\"+str(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A separate cell creates a Perceptron\n",
    "pSigmoid = pSigmoid(28*28+1)\n",
    "pSigmoid.print_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the node\n",
    "print(\"Training...\")\n",
    "training_data = train_data\n",
    "labels = [d[0]==0 for d in train_labels]\n",
    "pSigmoid.train(training_data, labels)\n",
    "print(\"Complete.\")\n",
    "\n",
    "# Testing the node\n",
    "print(\"Testing...\")\n",
    "testing_data = test_data\n",
    "labels = [d[0]==0 for d in test_labels]\n",
    "pSigmoid.test(testing_data, labels)\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** <br>\n",
    "Using the sigmoid activation function and online learning, the perceptron achieved an accuracy of 0.9594, which is marginally higher than the accuracy achieved by both the perceptron using step activation (0.9462) and full batch learning (0.9475). The precision of the sigmoid model was 0.8544, which is significantly higher than the precision achieved by the perceptron using step activation function and online learning (0.6708) and slightly higher than the precision achieved by the perceptron using step activation function and full batch learning (0.7367). This means the sigmoid perceptron identified false positives 85.44% of the time. \n",
    "\n",
    "The recall of the sigmoid model was 0.716, which is lower than the recall achieved by the perceptron using step activation function and online learning (0.907) and slightly lower than the recall achieved by the perceptron using step activation function and full batch learning (0.739). This means that the sigmoid perceptron correctly identified true positives 71.6% of the time.\n",
    "\n",
    "Overall, the sigmoid function outperformed both perceptron models in terms of accuracy and precision, but had a lower recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5. Print weights and Fashion MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_file = open('class_names_fashion.json')\n",
    "class_names = json.load(class_names_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_data[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References \n",
    "\n",
    "Alpaydin, E. (2020) Introduction to machine learning. 3rd edn. London, England: The Mit Press. <br>\n",
    "Pratiwi, H. and Rahadjeng, I.R. (2020) “Sigmoid activation function in selecting the best model of Artificial Neural Networks,” Journal of Physics: Conference Series, 1471(1), p. 012010. Available at: https://doi.org/10.1088/1742-6596/1471/1/012010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dddedaa35bad3fac85e6b892adc5ae6a6e37a1f5099d423ec882876c2ee1a6ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
